# Import libraries for data wrangling, preprocessing and visualization
from difflib import SequenceMatcher
from sklearn.preprocessing import MultiLabelBinarizer
from keras.models import Sequential
from keras.layers import Dense
import numpy as np
import pandas as pd
import category_encoders as ce
import os

class Learn(object):

    def __init__(self):
        self.load()

    data = []

    # Determines similarity of strings--used for generalizing hostnames and other attributes during onehot encoding
    def similar(self, a, b):
        return SequenceMatcher(None, a, b).ratio()

    # Compares first letters of strings for special cases wherein same bases indicates similar attributes
    def first_letters(a, b):
        vals = [.5, .3, .25, -.1]
        for n in range(0, 4):
            c = 3 - n
            sub_a = a[:c]
            sub_b = b[:c]
            if SequenceMatcher(None, sub_a, sub_b).ratio() == 1.0:
                return vals[n]

    # Takes a list of words, then categorizes them into mini "similar" lists, based on the categorizing functions above
    def group_words(self, list_words):
        total = list_words
        similar_lists = []
        all_words = total
        while len(total) > 0:

            for word in all_words:
                if word in total:
                    sim = list()
                    sim.append(word)
                    total = np.delete(total, np.where(total == word))
                    for comp_word in total:
                        # Calculates ratio of similarity, threshold is currently set to .6
                        if self.similar(word, comp_word) + self.first_letters(word, comp_word) > .6:
                            sim.append(comp_word)
                            # Adds word to "similar" list, then deletes it from the primary list
                            total = np.delete(total, np.where(total == comp_word))
                    similar_lists.append(sim)
                else:
                    continue
                    # Word is already in category

        # Returns one list, containing other smaller lists (of categorized words)
        return similar_lists

    # End String Sorting----------------------
    def prediction(self, single_line):
        # Single line will be used to predict at this point
        # Load model

        # Read input line into csv (concrete path, does not change)
        file_csv = '/bb/data/dcreaper/ml/training_data.csv'
        with open(file_csv, 'a') as file:
            file.write(('\n' + ','.join(single_line)) + ',0')
            file.close()

        full = load(file_csv, False)  # Do not train, only using for data conversion
        with open('/bb/data/dcreaper/ml/length.txt', 'r') as file:
            num_len = file.readline()
            file.close()
        model = None
        try:
            model = create_baseline(int(num_len))
            model.load_weights('/bb/data/dcreaper/reaper_model.h5')
        except Exception as e:
            print(e)
        # last line of new data from full data
        last_full = full[-1]
        # training_line=np.array([last_full[:-1]])

        with open(file_csv, "r") as f:
            lines = f.readlines()
            lines = lines[:-1]
            f.close()
        try:
            os.remove(file_csv)
        except Exception as e:
            print(e)
        with open(file_csv, "w", newline='') as f:
            for l in lines:
                if l is not '':
                    f.write(l)
            f.close()

        training_line = np.array([last_full])
        ones = False
        training_num = int(training_line.shape[1])
        while int(num_len) != training_num:

            if int(num_len) > training_num:
                training_line = np.array([np.append(training_line[0], 0)])
            else:
                if training_line[0][-1] == 1:
                    ones = True
                training_line = np.delete(training_line[0], -1)
                training_line = np.array([training_line])

            training_num = int(training_line.shape[1])
        if ones:
            training_line = np.delete(training_line[0], -1)
            training_line = np.array([training_line])
            training_line = np.array([np.append(training_line[0], 1)])

        pred = model.predict(training_line)
        try:
            return pred[0][0]
        except Exception as E:
            print(E)

        return -1

    # Creates the actual outline and framework for the model, does not train
    def create_baseline(self, current_size):
        # create model
        model = Sequential()
        model.add(Dense(15, input_dim=current_size, kernel_initializer='normal', activation='relu'))
        model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))
        # Compile model. We use the the logarithmic loss function, and the Adam gradient optimizer.
        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
        return model

    # Trains the model based on current data
    def train_model(self, x, y):
        model = self.create_baseline(len(x[0]))
        model.fit(x, y, epochs=100, batch_size=5, verbose=1)
        # Fits model according to x and y, training data passed in
        loss, acc_test = model.evaluate(x, y)
        print(acc_test)
        print(loss)
        model.save('/bb/data/dcreaper/reaper_model.h5')
        # Saves the model in memory
        with open('/bb/data/dcreaper/ml/length.txt', 'w') as file:
            # Records the current length of the training data in a text file
            file.write(str(len(x[0])))
            file.close()
        return model


    # Read data file
    def load(self, filen, train):
        self.data = pd.read_csv(filen, header=0)
        seed = 5
        np.random.seed(seed)

        # Select the columns to use for prediction in the neural network
        # prediction_var = ['hostname', 'cluster', 'parentcluster', 'cloud', 'spec', 'manu', 'model', 'building', 'owners',
        #                 'tags', 'purpose']
        # x = data[prediction_var].values
        y = data.under_utilized.values

        # One-Hot Encoding------------------------------------------
        one_hot = [data.cloud.values, data.building.values, data.parentcluster.values, data.spec.values, data.manu.values,
                   data.purpose]
        n = 0
        onehot_narr = [[]]

        for big_col in one_hot:
            one_hot_data = []
            for col in big_col:
                one_hot_data.append([col])
            # End for loop

            onehotencoder = ce.OneHotEncoder(cols=[0])
            one_hot_encoded_data = onehotencoder.fit_transform(one_hot_data, y)
            if n != 0:
                onehot_narr = np.concatenate((onehot_narr, one_hot_encoded_data), axis=1)
            else:
                onehot_narr = one_hot_encoded_data
                # order+=one_hot_encoded_data.classes_
            n += 1

        # End One-hot. Data stored in: onehot_nArr-------------------------------------


    def multi_label_binary_encoding(self):
        # MultiLabelBinary Encoding-------------------------------------------
        n = 0
        multilabel_arr = [[]]
        multi_data = [self.data.owners.values, self.data.tags.values]

        for col in multi_data:
            multi_fixed = []
            col_arr = []
            for strings in col:
                if strings is not 'null':
                    core = str(strings)[1:-1]  # Cuts off brackets
                    mini_row = core.split(' ')
                    mini_row = [c for c in mini_row if c is not '']
                    col_arr.append(mini_row)
                else:
                    col_arr.append(strings)
            multi_fixed.append(col_arr)

            for multi_col in multi_fixed:
                mlb = MultiLabelBinarizer()
                mlb_arr = mlb.fit_transform(multi_col)
                if n != 0:
                    multilabel_arr = np.concatenate((multilabel_arr, mlb_arr), axis=1)
                else:
                    multilabel_arr = mlb_arr
                n += 1
        return multilabel_arr
        # MultiLabelBinary Encoding end. Data stored in: multiLabel_arr-----------------


    def string_encoding(self):
        # String encoding-------------------------------------
        string_data = [self.data.hostname.values, self.data.cluster.values, self.data.model.values]
        full_str_enc = [[]]
        i = 0
        for col in string_data:
            orig_total = col
            grouped = group_words(col)
            for group in grouped:
                for word in group[1:]:
                    word += ''
                    orig_total = [group[0] if x in group else x for x in orig_total]
            onehotencoder = ce.OneHotEncoder(cols=[0])

            orig_total = onehotencoder.fit_transform(orig_total, y)
            if i != 0:
                full_str_enc = np.concatenate((full_str_enc, orig_total), axis=1)
            else:
                full_str_enc = orig_total
            i += 1
        return full_str_enc
        # End String encoding----------------------------------


    # Combine Data
    def combine(self, onehot_narr, multilabel_arr, full_str_enc, train):
        full_set = onehot_narr
        res_col = []
        for x in self.data.under_utilized.values:
            res_col.append([x])
        full_set = np.concatenate((full_set, multilabel_arr, full_str_enc), axis=1)
        full_set = np.concatenate((full_set, res_col), axis=1)  # Finally, adds in results
        x = full_set
        load.si = full_set.shape[1]
        if train is True:
            train_model(x, y)

        return full_set
        # Load model into pickle file from here to save it!!!


"""
load('/bb/data/dcreaper/ml/training_data.csv', True)
prediction(['biewnjpqsnen','qsenp','qsenc','prod','2017-c','genuineintel',
    'proliant dl360 gen9','ridge road','[]','[MSWIN RIDGE S4 S4B WINSA TIER4 CTMNA CTM04 QSENP QSENC]','1'])
prediction(['njpix11','branch','msadds','prod','2019edggfwe3-c','hews','eriko2121',
    'ba sing se','null','[BBRC ORANGE NANO MINI RTCPIX]','1'])
prediction(['nyks1','devrdc','msadds','null','d','hp','proliant dl380p gen9','orangeburg',
    '[6967290  6550146  5025281  4178832  7555170]','[ORANGE]','0'])
prediction(['njphatqa6t','bdmz','mstss','null','','hp','proliant dl380p gen9','ridge road','[6550146]','null','0'])
prediction(['wsawnjpadsdc01','msgsmb','msgpsm','c','d','dell','proliant dl360p gen8',
    'ridge road','[15290763  7034656  10835025  7975480]','null','1'])
prediction(['reaction','how','woll','c','dfdwc','erik','nullll','erkd','[32323232]', 'null', '0'])
prediction(['bmsepd-rr-043','bmsepd','bmse','None','2017-c','genuineintel',
    'proliant dl360 gen9','ridge road','[7477039]',
BBCPU BBRC BPKG CTM15 CTMNA LINUX LNXDEV LNXSA MINI NANO NOTRST OBTOFF PDCLD PRCCHK RIDGE RTCPUX
    ALMN SDCLD BMS EPD BMSE]','0'])
prediction(['bbcsd1-rr-521','deltoid','msadds','eriko','c','hp1',
'proliant dl360 gen9','ridege road','[6550146]','null','1'])
prediction(['bldbot-ob-139','bldbot','msadds','prod','2018-c','genuineintel','proliant dl360 gen9','orangeburg',
'[241417 14503697  8225]','[ORANGE PDCLD RAND SN1 SN1D WEST WINSA CTM06 CTMNA JSCIWIN BLDBOT AFCI]','0'])
"""
